{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xor resolver using neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a simple `xor` solver using neural network from scrab\n",
    "\n",
    "![](https://i.ytimg.com/vi/yTFc7uCZG5k/maxresdefault.jpg)\n",
    "\n",
    "\n",
    "![](https://i.ytimg.com/vi/kNPGXgzxoHw/maxresdefault.jpg)\n",
    "\n",
    "![](https://www.researchgate.net/profile/Medhat_Moussa/publication/228939274/figure/fig1/AS:393876184551431@1470918808455/Topology-of-ANN-used-to-solve-logic-XOR-problem.png)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*qA_APGgbbh0QfRNsRyMaJg.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:\n",
      " \n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "\n",
      "\n",
      "Output Y:\n",
      " \n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# define the inputs\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([0, 1, 1, 0]).T\n",
    "\n",
    "print(\"Input X:\\n \\n{0}\\n\\n\".format(X))\n",
    "print(\"Output Y:\\n \\n{0}\".format(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input is matrix with 4 rows and 2 columns.\n",
      "size of the hidden layer is 2\n",
      "Leanring rate is 1\n",
      "\n",
      "Initial weights for layer 1 are:\n",
      "[[ 0.99288704  0.22124904]\n",
      " [ 0.38880011  0.73799269]\n",
      " [ 0.16169034  0.21241999]]\n",
      "\n",
      "Initial weights for layer 2 are:\n",
      "[[ 0.39114288]\n",
      " [ 0.07573942]\n",
      " [ 0.91038496]]\n"
     ]
    }
   ],
   "source": [
    "# define hypepramaters\n",
    "\n",
    "m, n = X.shape\n",
    "hidden_size = 2\n",
    "learning_rate = 1\n",
    "number_iteration = 1000\n",
    "\n",
    "weights_1 = np.random.random( (n + 1 , hidden_size))\n",
    "weights_2 = np.random.random( (hidden_size + 1, 1))\n",
    "\n",
    "print(\"Our input is matrix with {0} rows and {1} columns.\".format(m, n))\n",
    "print(\"size of the hidden layer is {0}\".format(hidden_size))\n",
    "print(\"Leanring rate is {0}\".format(learning_rate))  \n",
    "\n",
    "print(\"\\nInitial weights for layer 1 are:\\n{0}\".format(weights_1))\n",
    "\n",
    "print(\"\\nInitial weights for layer 2 are:\\n{0}\".format(weights_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, weights_1, weights_2):\n",
    "    X_bias_1 = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    results_1 = X_bias_1.dot(weights_1)\n",
    "    \n",
    "    activate_values_1 = sigmoid(results_1)\n",
    "    \n",
    "    X_bias_2 =  np.c_[np.ones(X.shape[0]), results_1]\n",
    "    \n",
    "    results_2 = X_bias_2.dot(weights_2)\n",
    "    \n",
    "    activate_values_2 = sigmoid(results_2)\n",
    "    \n",
    "    return results_1, activate_values_1, results_2, activate_values_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of the layer 1 : \n",
      " [[ 0.99288704  0.22124904]\n",
      " [ 1.15457737  0.43366903]\n",
      " [ 1.38168715  0.95924173]\n",
      " [ 1.54337749  1.17166172]]\n",
      "\n",
      "results of the layer 1 activate function: \n",
      " [[ 0.27034221  0.44491227]\n",
      " [ 0.239654    0.39325054]\n",
      " [ 0.20073817  0.27703004]\n",
      " [ 0.17604482  0.23655475]]\n",
      "\n",
      "results of the layer 2 : \n",
      " [[ 0.66776536]\n",
      " [ 0.87339565]\n",
      " [ 1.3690703 ]\n",
      " [ 1.5747006 ]]\n",
      "\n",
      "\n",
      "results of the layer 2 activate function: \n",
      " [[ 0.3389974 ]\n",
      " [ 0.29454823]\n",
      " [ 0.20277009]\n",
      " [ 0.17154732]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_1, activate_values_1, results_2, activate_values_2   = feed_forward(X, weights_1, weights_2)\n",
    "\n",
    "print(\"results of the layer 1 : \\n {0}\\n\".format(results_1))\n",
    "print(\"results of the layer 1 activate function: \\n {0}\\n\".format(activate_values_1))\n",
    "\n",
    "print(\"results of the layer 2 : \\n {0}\\n\\n\".format(results_2))\n",
    "print(\"results of the layer 2 activate function: \\n {0}\\n\\n\".format(activate_values_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X, Y, weights_1, weights_2, lraning_rate, number_iteration):\n",
    "    \n",
    "    current_weights_1 = weights_1\n",
    "    current_weights_2 = weights_2\n",
    "    \n",
    "    for i in range(number_iteration):\n",
    "        h1, alpha1, h2, alpha2 = feed_forward(\n",
    "            X,\n",
    "            current_weights_1,\n",
    "            current_weights_2\n",
    "        )\n",
    "        # loss fn = MSE\n",
    "        t = np.array([Y]).T\n",
    "        \n",
    "        e2 = (h2 - t) / X.shape[1]\n",
    "        \n",
    "        dw2 = np.dot(sigmoid_grad(alpha2), e2.T)\n",
    "        \n",
    "        e1 = np.dot(dw2, current_weights_2)\n",
    "        \n",
    "        dw1 = np.dot(sigmoid_grad(alpha), e1.T )\n",
    "        \n",
    "        layer1_adjustment = X.T.dot(dw1)\n",
    "        layer2_adjustment = Y.T.dot(dw2)\n",
    "        \n",
    "        current_weights_2 += (lraning_rate * layer2_adjustment)\n",
    "        current_weights_1 += (lraning_rate * layer1_adjustment)\n",
    "        \n",
    "    return current_weights_1, current_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-94081731772c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mweights_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnumber_iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a88f5ef1706b>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(X, Y, weights_1, weights_2, lraning_rate, number_iteration)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcurrent_weights_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mcurrent_weights_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         )\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# loss fn = MSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": [
    "final_weights_1, final_wights_2 = back_propagation(\n",
    "    X,\n",
    "    Y,\n",
    "    weights_1,\n",
    "    weights_2,\n",
    "    learning_rate,\n",
    "    number_iteration\n",
    ")\n",
    "\n",
    "\n",
    "print(\"the errors_graph: \\n {0}\\n\".format(errors_graph))\n",
    "print(\"the final weights of layer 1: \\n {0}\\n\".format(final_weights_1))\n",
    "print(\"the final weights of layer 2: \\n {0}\\n\".format(final_wights_2))\n",
    "\n",
    "\n",
    "_, _, _, _, predicted_values = feed_forward(X, final_weights_1, final_wights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chrisjmccormick.files.wordpress.com/2014/03/gradientdescentofmsetable.png)\n",
    "[https://www.slideshare.net/Jboulie/ann-preso-draft](https://www.slideshare.net/Jboulie/ann-preso-draft)\n",
    "\n",
    "[http://mccormickml.com/2014/03/04/gradient-descent-derivation/](http://mccormickml.com/2014/03/04/gradient-descent-derivation/)\n",
    "\n",
    "[https://iamtrask.github.io/2015/07/27/python-network-part2/](https://iamtrask.github.io/2015/07/27/python-network-part2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
