{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xor resolver using neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a simple `xor` solver using neural network from scrab\n",
    "\n",
    "![](https://i.ytimg.com/vi/yTFc7uCZG5k/maxresdefault.jpg)\n",
    "\n",
    "\n",
    "![](https://i.ytimg.com/vi/kNPGXgzxoHw/maxresdefault.jpg)\n",
    "\n",
    "![](https://www.researchgate.net/profile/Medhat_Moussa/publication/228939274/figure/fig1/AS:393876184551431@1470918808455/Topology-of-ANN-used-to-solve-logic-XOR-problem.png)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*qA_APGgbbh0QfRNsRyMaJg.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:\n",
      " \n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "\n",
      "\n",
      "Output Y:\n",
      " \n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# define the inputs\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([0, 1, 1, 0]).T\n",
    "\n",
    "print(\"Input X:\\n \\n{0}\\n\\n\".format(X))\n",
    "print(\"Output Y:\\n \\n{0}\".format(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input is matrix with 4 rows and 2 columns.\n",
      "size of the hidden layer is 2\n",
      "Leanring rate is 1\n",
      "\n",
      "Initial weights for layer 1 are:\n",
      "[[ 0.70226723  0.90029796]\n",
      " [ 0.76050628  0.71859573]\n",
      " [ 0.19187847  0.28666266]]\n",
      "\n",
      "Initial weights for layer 2 are:\n",
      "[[ 0.47421091]\n",
      " [ 0.0243439 ]\n",
      " [ 0.877824  ]]\n"
     ]
    }
   ],
   "source": [
    "# define hypepramaters\n",
    "\n",
    "m, n = X.shape\n",
    "hidden_size = 2\n",
    "learning_rate = 1\n",
    "number_iteration = 1000\n",
    "\n",
    "weights_1 = np.random.random( (n + 1 , hidden_size))\n",
    "weights_2 = np.random.random( (hidden_size + 1, 1))\n",
    "\n",
    "print(\"Our input is matrix with {0} rows and {1} columns.\".format(m, n))\n",
    "print(\"size of the hidden layer is {0}\".format(hidden_size))\n",
    "print(\"Leanring rate is {0}\".format(learning_rate))  \n",
    "\n",
    "print(\"\\nInitial weights for layer 1 are:\\n{0}\".format(weights_1))\n",
    "print(\"\\nInitial weights for layer 2 are:\\n{0}\".format(weights_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    y = sigmoid(x)\n",
    "    \n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, weights_1, weights_2):\n",
    "    X_bias_1 = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    results_1 = X_bias_1.dot(weights_1)\n",
    "    \n",
    "    activate_values_1 = sigmoid(results_1)\n",
    "    \n",
    "    X_bias_2 =  np.c_[np.ones(X.shape[0]), results_1]\n",
    "    \n",
    "    results_2 = X_bias_2.dot(weights_2)\n",
    "    \n",
    "    activate_values_2 = sigmoid(results_2)\n",
    "    \n",
    "    return X_bias_1, results_1, activate_values_1, X_bias_2, results_2, activate_values_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input with bias of layer 1: \n",
      " [[ 1.  0.  0.]\n",
      " [ 1.  0.  1.]\n",
      " [ 1.  1.  0.]\n",
      " [ 1.  1.  1.]]\n",
      "\n",
      "results of the layer 1 : \n",
      " [[ 0.70226723  0.90029796]\n",
      " [ 0.8941457   1.18696062]\n",
      " [ 1.4627735   1.61889369]\n",
      " [ 1.65465198  1.90555635]]\n",
      "\n",
      "results of the layer 1 activate function: \n",
      " [[ 0.33130975  0.28898927]\n",
      " [ 0.29025504  0.23380297]\n",
      " [ 0.18804349  0.1653575 ]\n",
      " [ 0.16048121  0.1294809 ]]\n",
      "\n",
      "input with bias of layer 2: \n",
      " [[ 1.          0.70226723  0.90029796]\n",
      " [ 1.          0.8941457   1.18696062]\n",
      " [ 1.          1.4627735   1.61889369]\n",
      " [ 1.          1.65465198  1.90555635]]\n",
      "\n",
      "results of the layer 2 : \n",
      " [[ 1.28160998]\n",
      " [ 1.53792042]\n",
      " [ 1.93092425]\n",
      " [ 2.18723468]]\n",
      "\n",
      "\n",
      "results of the layer 2 activate function: \n",
      " [[ 0.21727629]\n",
      " [ 0.17683779]\n",
      " [ 0.12664832]\n",
      " [ 0.10090269]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_bias_1, results_1, activate_values_1,  X_bias_2, results_2, activate_values_2   = feed_forward(X, weights_1, weights_2)\n",
    "\n",
    "print(\"input with bias of layer 1: \\n {0}\\n\".format(X_bias_1))\n",
    "print(\"results of the layer 1 : \\n {0}\\n\".format(results_1))\n",
    "print(\"results of the layer 1 activate function: \\n {0}\\n\".format(activate_values_1))\n",
    "\n",
    "print(\"input with bias of layer 2: \\n {0}\\n\".format(X_bias_2))\n",
    "print(\"results of the layer 2 : \\n {0}\\n\\n\".format(results_2))\n",
    "print(\"results of the layer 2 activate function: \\n {0}\\n\\n\".format(activate_values_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X, Y, weights_1, weights_2, lraning_rate, number_iteration):\n",
    "    \n",
    "    current_weights_1 = weights_1\n",
    "    current_weights_2 = weights_2\n",
    "    errors_graph = np.array([])\n",
    "    \n",
    "    for i in range(number_iteration):\n",
    "        x1, z1, alpha1, x2, z2, alpha2 = feed_forward(\n",
    "            X,\n",
    "            current_weights_1,\n",
    "            current_weights_2\n",
    "        )\n",
    "        # loss fn = MSE\n",
    "        t = np.array([Y]).T\n",
    "        \n",
    "        layer_2_delta = (alpha2 - t)*(alpha2*(1-alpha2))\n",
    "        \n",
    "        print(layer_2_delta)\n",
    "        print(current_weights_1)\n",
    "        \n",
    "        layer_1_delta = layer_2_delta.dot(current_weights_1.T) * (alpha1 * (1-alpha1))\n",
    "        \n",
    "        current_weights_2 -= (lraning_rate * alpha1.T.dot(layer_2_delta))\n",
    "        current_weights_1 -= (lraning_rate * X.T.dot(layer_1_delta))\n",
    "        \n",
    "    return current_weights_1, current_weights_2, errors_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03695159]\n",
      " [-0.11982458]\n",
      " [-0.09660014]\n",
      " [ 0.00915403]]\n",
      "[[ 0.70226723  0.90029796]\n",
      " [ 0.76050628  0.71859573]\n",
      " [ 0.19187847  0.28666266]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,1) and (2,3) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-57973012a74d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mweights_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnumber_iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-571ac6870b13>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(X, Y, weights_1, weights_2, lraning_rate, number_iteration)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_weights_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mlayer_1_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_2_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_weights_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcurrent_weights_2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlraning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_2_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,1) and (2,3) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "final_weights_1, final_wights_2, errors_graph = back_propagation(\n",
    "    X,\n",
    "    Y,\n",
    "    weights_1,\n",
    "    weights_2,\n",
    "    learning_rate,\n",
    "    number_iteration\n",
    ")\n",
    "\n",
    "\n",
    "print(\"the errors_graph: \\n {0}\\n\".format(errors_graph))\n",
    "print(\"the final weights of layer 1: \\n {0}\\n\".format(final_weights_1))\n",
    "print(\"the final weights of layer 2: \\n {0}\\n\".format(final_wights_2))\n",
    "\n",
    "\n",
    "_, _, _, _, predicted_values = feed_forward(X, final_weights_1, final_wights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chrisjmccormick.files.wordpress.com/2014/03/gradientdescentofmsetable.png)\n",
    "[https://www.slideshare.net/Jboulie/ann-preso-draft](https://www.slideshare.net/Jboulie/ann-preso-draft)\n",
    "\n",
    "[http://mccormickml.com/2014/03/04/gradient-descent-derivation/](http://mccormickml.com/2014/03/04/gradient-descent-derivation/)\n",
    "\n",
    "[https://iamtrask.github.io/2015/07/27/python-network-part2/](https://iamtrask.github.io/2015/07/27/python-network-part2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
