{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xor resolver using neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a simple `xor` solver using neural network from scrab\n",
    "\n",
    "![](https://i.ytimg.com/vi/yTFc7uCZG5k/maxresdefault.jpg)\n",
    "\n",
    "\n",
    "![](https://i.ytimg.com/vi/kNPGXgzxoHw/maxresdefault.jpg)\n",
    "\n",
    "![](https://www.researchgate.net/profile/Medhat_Moussa/publication/228939274/figure/fig1/AS:393876184551431@1470918808455/Topology-of-ANN-used-to-solve-logic-XOR-problem.png)\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*qA_APGgbbh0QfRNsRyMaJg.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X:\n",
      " \n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "\n",
      "\n",
      "Output Y:\n",
      " \n",
      "[0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# define the inputs\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([0, 1, 1, 0]).T\n",
    "\n",
    "print(\"Input X:\\n \\n{0}\\n\\n\".format(X))\n",
    "print(\"Output Y:\\n \\n{0}\".format(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input is matrix with 4 rows and 2 columns.\n",
      "size of the hidden layer is 2\n",
      "Leanring rate is 1\n",
      "\n",
      "Initial weights for layer 1 are:\n",
      "[[ 0.31494747  0.51661395]\n",
      " [ 0.60902069  0.40237811]\n",
      " [ 0.24663124  0.72622975]]\n",
      "\n",
      "Initial weights for layer 2 are:\n",
      "[[ 0.51746877]\n",
      " [ 0.23014433]\n",
      " [ 0.54451938]]\n"
     ]
    }
   ],
   "source": [
    "# define hypepramaters\n",
    "\n",
    "m, n = X.shape\n",
    "hidden_size = 2\n",
    "learning_rate = 1\n",
    "number_iteration = 1000\n",
    "\n",
    "weights_1 = np.random.random( (n + 1 , hidden_size))\n",
    "weights_2 = np.random.random( (hidden_size + 1, 1))\n",
    "\n",
    "print(\"Our input is matrix with {0} rows and {1} columns.\".format(m, n))\n",
    "print(\"size of the hidden layer is {0}\".format(hidden_size))\n",
    "print(\"Leanring rate is {0}\".format(learning_rate))  \n",
    "\n",
    "print(\"\\nInitial weights for layer 1 are:\\n{0}\".format(weights_1))\n",
    "print(\"\\nInitial weights for layer 2 are:\\n{0}\".format(weights_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    y = sigmoid(x)\n",
    "    \n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X, weights_1, weights_2):\n",
    "    X_bias_1 = np.c_[np.ones(X.shape[0]), X]\n",
    "    \n",
    "    results_1 = X_bias_1.dot(weights_1)\n",
    "    \n",
    "    X_bias_2 =  np.c_[np.ones(X.shape[0]), results_1]\n",
    "    \n",
    "    results_2 = X_bias_2.dot(weights_2)\n",
    "    \n",
    "    activate_values = sigmoid(results_2)\n",
    "    \n",
    "    return X_bias_1, results_1, X_bias_2, results_2, activate_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input with bias of layer 1: \n",
      " [[ 1.  0.  0.]\n",
      " [ 1.  0.  1.]\n",
      " [ 1.  1.  0.]\n",
      " [ 1.  1.  1.]]\n",
      "\n",
      "results of the layer 1 : \n",
      " [[ 0.31494747  0.51661395]\n",
      " [ 0.56157871  1.2428437 ]\n",
      " [ 0.92396816  0.91899206]\n",
      " [ 1.1705994   1.64522181]]\n",
      "\n",
      "\n",
      "input with bias of layer 2: \n",
      " [[ 1.          0.31494747  0.51661395]\n",
      " [ 1.          0.56157871  1.2428437 ]\n",
      " [ 1.          0.92396816  0.91899206]\n",
      " [ 1.          1.1705994   1.64522181]]\n",
      "\n",
      "results of the layer 2 : \n",
      " [[ 0.87125845]\n",
      " [ 1.3234654 ]\n",
      " [ 1.23052378]\n",
      " [ 1.68273073]]\n",
      "\n",
      "\n",
      "results of the activate function: \n",
      " [[ 0.29499251]\n",
      " [ 0.21024232]\n",
      " [ 0.22608977]\n",
      " [ 0.15673421]]\n"
     ]
    }
   ],
   "source": [
    "X_bias_1, results_1, X_bias_2, results_2, activate_values   = feed_forward(X, weights_1, weights_2)\n",
    "\n",
    "print(\"input with bias of layer 1: \\n {0}\\n\".format(X_bias_1))\n",
    "print(\"results of the layer 1 : \\n {0}\\n\\n\".format(results_1))\n",
    "\n",
    "print(\"input with bias of layer 2: \\n {0}\\n\".format(X_bias_2))\n",
    "print(\"results of the layer 2 : \\n {0}\\n\\n\".format(results_2))\n",
    "\n",
    "print(\"results of the activate function: \\n {0}\".format(activate_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X, Y, weights_1, weights_2, lraning_rate, number_iteration):\n",
    "    \n",
    "    current_weights_1 = weights_1\n",
    "    current_weights_2 = weights_2\n",
    "    errors_graph = np.array([])\n",
    "    \n",
    "    for i in range(number_iteration):\n",
    "        X_bias_1, results_1, X_bias_2, results_2, activate_values = feed_forward(\n",
    "            X,\n",
    "            current_weights_1,\n",
    "            current_weights_2\n",
    "        )\n",
    "        \n",
    "        error = np.subtract(np.array([Y]).T, activate_values)\n",
    "        \n",
    "        print(error)\n",
    "        \n",
    "        delta_2 = error.dot(weights_2)\n",
    "        \n",
    "        current_weights_2 += lraning_rate * X_bias_2.T.dot(current_weights_2)\n",
    "        current_weights_1 += lraning_rate * X_bias_1.T.dot(current_weights_1)\n",
    "        \n",
    "    return current_weights_1, current_weights_2, errors_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29499251]\n",
      " [ 0.78975768]\n",
      " [ 0.77391023]\n",
      " [-0.15673421]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-57973012a74d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mweights_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnumber_iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-a944deb2ba74>\u001b[0m in \u001b[0;36mback_propagation\u001b[0;34m(X, Y, weights_1, weights_2, lraning_rate, number_iteration)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdelta_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcurrent_weights_2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlraning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX_bias_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_weights_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "final_weights_1, final_wights_2, errors_graph = back_propagation(\n",
    "    X,\n",
    "    Y,\n",
    "    weights_1,\n",
    "    weights_2,\n",
    "    learning_rate,\n",
    "    number_iteration\n",
    ")\n",
    "\n",
    "\n",
    "print(\"the errors_graph: \\n {0}\\n\".format(errors_graph))\n",
    "print(\"the final weights of layer 1: \\n {0}\\n\".format(final_weights_1))\n",
    "print(\"the final weights of layer 2: \\n {0}\\n\".format(final_wights_2))\n",
    "\n",
    "\n",
    "_, _, _, _, predicted_values = feed_forward(X, final_weights_1, final_wights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
